# NLP-FinalProject

### Probing Linguistic Knowledge In Non Latin Script Neural Language Models

The project will explore the linguistic knowledge implicitly learned by character-based recurrent neural networks (RNN) and its variants including long short-term memory (LSTM) and gated recurrent units (GRU). It will compare these character-based architectures with their word-based level counterparts. The project will focus on understanding morphology and syntactic dependencies in the following languages: English, Italian, Arabic, and Hindi by probing the networks using a correlation analysis technique called "diagnostic classifier"
